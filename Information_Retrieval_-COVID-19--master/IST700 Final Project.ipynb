{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.8.tar.gz (981 kB)\n",
      "Requirement already satisfied: six in e:\\py\\lib\\site-packages (from langdetect) (1.12.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.8-py3-none-any.whl size=993197 sha256=5a20f0c7f07cafd464096f2ac989fcf6da686d310f0611f6efcc7716b0076d4e\n",
      "  Stored in directory: c:\\users\\krishna\\appdata\\local\\pip\\cache\\wheels\\59\\f6\\9d\\85068904dba861c0b9af74e286265a08da438748ee5ae56067\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n",
    "!pip install semantic-text-similarity\n",
    "\n",
    "#Libraries needed\n",
    "import pandas as pd \n",
    "import glob\n",
    "import json\n",
    "import re \n",
    "import numpy as np\n",
    "import copy \n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "from langdetect import detect\n",
    "from semantic_text_similarity.models import ClinicalBertSimilarity\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the saved data \n",
    "print(\"Loading the dataframe.\")\n",
    "df_covid = pd.read_csv('covidData.csv')\n",
    "print(\"Dataframe loaded.\")\n",
    "print()\n",
    "df_covid = df_covid.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all articles that have fewer than the number of words specified \n",
    "min_word_count = 1000\n",
    "print(\"Removing all articles with fewer than \"+str(min_word_count)+\" words.\")\n",
    "indexNames = df_covid[df_covid['body_word_count'] < min_word_count].index\n",
    "df_covid = df_covid.drop(indexNames)\n",
    "df_covid = df_covid.reset_index(drop=True)\n",
    "print(\"Articles cleaned.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all non-English articles\n",
    "print(\"Removing all non-English articles\")\n",
    "index = 0\n",
    "indexNames = []\n",
    "while(index < len(df_covid)):\n",
    "    print(f'Processing index: {index} of {len(df_covid)}', end='\\r')\n",
    "    language = detect(df_covid.iloc[index]['body_text'])\n",
    "    if(language != 'en'):\n",
    "        indexNames.append(index)\n",
    "    index += 1\n",
    "df_covid = df_covid.drop(indexNames)\n",
    "df_covid = df_covid.reset_index(drop=True)\n",
    "print(\"All non-English articles removed. Total article count is now: \"+str(len(df_covid)))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the cleaned dataset \n",
    "print(\"Saving the dataframe.\")\n",
    "df_covid.to_csv('covidDataCleaned.csv') \n",
    "print(\"Dataframe saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the saved data \n",
    "print(\"Loading the dataframe.\")\n",
    "df_covid = pd.read_csv('covidDataCleaned.csv')\n",
    "print(\"Dataframe loaded.\")\n",
    "print()\n",
    "df_covid = df_covid.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the saved data \n",
    "print(\"Loading the dataframe.\")\n",
    "df_covid = pd.read_csv('../input/cord19cleaneddata/covidDataCleaned.csv')\n",
    "print(\"Dataframe loaded.\")\n",
    "print()\n",
    "df_covid = df_covid.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train & save the word2vec model \n",
    "print(\"Training word2vec.\")\n",
    "model = Word2Vec(sentences, size=100, window=5, min_count=10, workers=4)\n",
    "print(\"Word count:\", len(list(model.wv.vocab)))\n",
    "model.save(\"word2vec.model\")\n",
    "print(\"Finished training and saving word2vec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the trained word2vec model \n",
    "print(\"Loading the pre-trained word2vec model.\")\n",
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "print(\"Model loaded.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From: https://methodmatters.github.io/using-word2vec-to-analyze-word/\n",
    "#Define the function to compute the dimensionality reduction and then produce the biplot  \n",
    "def tsne_plot(model, words):\n",
    "    \"Creates a TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    \n",
    "    print(\"Getting embeddings.\")\n",
    "    for word in model.wv.vocab:  \n",
    "        if(word in words):\n",
    "            tokens.append(model[word])\n",
    "            labels.append(word)\n",
    "    print(\"Embeddings extracted.\")\n",
    "    print()\n",
    "        \n",
    "    print(\"Performing dimensionality reduction with t-sne.\")\n",
    "    tsne_model = TSNE(perplexity=5, n_components=2, init='pca', n_iter=2500, verbose=0)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    print(\"Dimensioanlity reduction complete.\")\n",
    "    print()\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i in range(len(x)):\n",
    "        if(labels[i] in words):\n",
    "            plt.scatter(x[i],y[i])\n",
    "            plt.annotate(labels[i], xy=(x[i], y[i]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "    plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of words to visualize in the plot\n",
    "words = ['china', 'italy', 'taiwan', 'india', 'japan', 'france', \n",
    "         'spain', 'canada', 'infection', 'disease', 'pathogen', \n",
    "         'organism', 'bacteria', 'virus', 'covid19', 'coronavirus', \n",
    "         'healthcase', 'doctor', 'nurse', 'specialist', 'hospital', \n",
    "         'novel', 'human', 'sars', 'covid', 'wuhan', 'case', \n",
    "         'background', 'dynamic', 'pneumonia', 'outbreak', 'pandemic', \n",
    "         'syndrome', 'contact', 'wash', 'hands', 'cough', \n",
    "         'respiratory', 'case', 'fear', 'spike', 'curve', \n",
    "         'transmission', 'seasonal', 'genome', 'dna', 'testing', \n",
    "         'asymptomatic', 'global', 'spread', 'diagnosis']\n",
    "  \n",
    "#Call the function on our dataset  \n",
    "tsne_plot(model, words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word to compare against and number of similar words to print out \n",
    "word = 'facemask'\n",
    "similarCount = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get and print the results \n",
    "results = model.wv.most_similar(positive=word, topn=similarCount)\n",
    "print(\"Input word:\", word)\n",
    "print(\"Top \"+str(similarCount)+\" similar words are:\")\n",
    "for index, word in enumerate(results):\n",
    "    print(str(index+1)+\". \"+word[0]+\" --- Score: \"+str(word[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words to compute cosine similarity over  \n",
    "word1 = 'china'\n",
    "word2 = 'wuhan'\n",
    "\n",
    "#Get the word embeddings \n",
    "embedding1 = model.wv[word1]\n",
    "embedding2 = model.wv[word2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the cosine similarity and print the results \n",
    "cosineSimilarity = np.sum(embedding1*embedding2) / (np.sqrt(np.sum(np.square(embedding1)))*np.sqrt(np.sum(np.square(embedding2))))\n",
    "print(\"Word1: \"+word1+\" --- Word2: \"+word2)\n",
    "print(\"Cosine similarity: \"+ str(cosineSimilarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the GPU device\n",
    "device = 0\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the saved data \n",
    "print(\"Loading the dataframe.\")\n",
    "df_covid = pd.read_csv('../input/cord19cleaneddata/covidDataCleaned.csv')\n",
    "print(\"Dataframe loaded.\")\n",
    "print()\n",
    "df_covid = df_covid.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable to store the batch size\n",
    "batchSize = 500 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the model\n",
    "print(\"Loading BERT semantic similarity model.\")\n",
    "model = ClinicalBertSimilarity(device='cuda', batch_size=batchSize) #defaults to GPU prediction\n",
    "print(\"Model loaded.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The primary questions that attempt to be answered  \n",
    "primaryQuestions = [\n",
    "    \"What is known about transmission, incubation, and environmental stability of coronavirus\"\n",
    "    #\"What do we know about coronavirus risk factors\"\n",
    "    #\"What do we know about coronavirus genetics, origin, and evolution\"\n",
    "    #\"What do we know about vaccines and therapeutics for coronavirus\"\n",
    "    #\"What has been published about coronavirus medical care\"\n",
    "    #\"What do we know about non-pharmaceutical interventions for coronavirus\"\n",
    "    #\"What do we know about diagnostics and surveillance of coronavirus\"\n",
    "    #\"In what ways does geography affects virality\"\n",
    "    #\"What has been published about ethical and social science considerations regarding coronavirus\"\n",
    "    #\"What has been published about information sharing and inter-sectoral collaboration\"\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
